{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from cs209b.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3072)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3072)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3072)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3072)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    # Validation set\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    # Training set\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    # Test set\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    # Dev data set: just for debugging purposes, it overlaps with the training set,\n",
    "    # but has a smaller size.\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "\n",
    "# Training parameters\n",
    "m_train = X_train.shape[0]   # number of training examples\n",
    "m_dev = X_dev.shape[0]       # number of training examples in the development set\n",
    "n = X_train.shape[1]         # features dimension\n",
    "c = 10                       # number of classes in the database\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "def encode_labels(y):\n",
    "    enc = OneHotEncoder()\n",
    "    enc.fit(y.reshape(-1,1))\n",
    "    y_enc = enc.transform(y.reshape(-1,1)).toarray()\n",
    "    return y_enc\n",
    "\n",
    "Y_train_enc = encode_labels(y_train)\n",
    "Y_val_enc =  encode_labels(y_val)\n",
    "Y_dev_enc = encode_labels(y_dev)\n",
    "Y_test_enc = encode_labels(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss_naive(W, b, X, Y, reg):\n",
    "    return softmax_loss_vectorized(W, b, X, Y, reg)\n",
    "\n",
    "def softmax_loss_vectorized(W, b, X, Y, reg):\n",
    "    \"\"\"\n",
    "    Softmax loss function, naive implementation (with loops)\n",
    "\n",
    "    Inputs have dimension D, there are C classes, and we operate on minibatches\n",
    "    of N examples.\n",
    "\n",
    "    Inputs:\n",
    "    - W: A numpy array of shape (c, n) containing weights.\n",
    "    - X: A numpy array of shape (m ,n) containing a minibatch of data.\n",
    "    - Y: A numpy array of shape (m, c) containing training labels using a one-hot\n",
    "            encoding, y[i, ci] = 1 means that X[i,:] has label ci.\n",
    "    - reg: (float) regularization strength\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss as single float\n",
    "    - dictionary of gradients with respect to W and b.\n",
    "    \"\"\"\n",
    "    c = W.shape[0]\n",
    "    n = W.shape[1]\n",
    "    m = X.shape[0]\n",
    "    # Initialize the loss and gradient to zero.\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)\n",
    "    db = np.zeros_like(b)\n",
    "\n",
    "    #############################################################################\n",
    "    # TODO: Compute the softmax loss and its gradient using explicit loops.     #\n",
    "    # Store the loss in loss and the gradient in dW. If you are not careful     #\n",
    "    # here, it is easy to run into numeric instability. Don't forget the        #\n",
    "    # regularization!                                                           #\n",
    "    #############################################################################\n",
    "    lin_sum = np.matmul(W, X.T) + np.tile(b, (1, m))\n",
    "    exp_sum = np.sum(np.exp(lin_sum), axis=0)\n",
    "    exp_sum = exp_sum.reshape((1, exp_sum.shape[0]))\n",
    "    softmax_matrix = np.exp(lin_sum) / np.tile(exp_sum, (c, 1))\n",
    "    ll_matrix = -np.log(softmax_matrix)\n",
    "#     pred_matrix = (softmax_matrix == softmax_matrix.max(axis=0, keepdims=1)).astype(float)\n",
    "    regularization = 0.5 * reg * np.square(LA.norm(W, ord = 'fro'))\n",
    "    loss = np.sum(np.multiply(Y.T, ll_matrix)) / m + 0.5*reg*np.sum(W**2)\n",
    "    dW = np.matmul((softmax_matrix - Y.T), X) / m + (reg*W)\n",
    "    db = np.matmul((softmax_matrix - Y.T), np.ones((m, 1)))/m\n",
    "\n",
    "\n",
    "    #############################################################################\n",
    "    #                          END OF YOUR CODE                                 #\n",
    "    #############################################################################\n",
    "    grads = {\"dW\": dW, \"db\": db}\n",
    "    return loss, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random loss:  2.339217\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(c,n) * 0.0001\n",
    "b = np.random.randn(c,1) * 0.0001\n",
    "loss = softmax_loss_naive(W, b, X_dev, Y_dev_enc, reg = 0.0)[0]\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('random loss:  %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical gradient of W without regularization:\n",
      "numerical: 1.949558 analytic: 1.949558, relative error: 1.275680e-10\n",
      "numerical: 0.770849 analytic: 0.770849, relative error: 9.591561e-12\n",
      "numerical: -5.912018 analytic: -5.912018, relative error: 1.658103e-11\n",
      "numerical: 1.448543 analytic: 1.448543, relative error: 2.290951e-10\n",
      "numerical: -0.795804 analytic: -0.795804, relative error: 4.670965e-10\n",
      "numerical: 0.726345 analytic: 0.726345, relative error: 8.126960e-10\n",
      "numerical: -0.816037 analytic: -0.816037, relative error: 3.205390e-10\n",
      "numerical: -0.256172 analytic: -0.256172, relative error: 3.963507e-09\n",
      "numerical: -0.182695 analytic: -0.182695, relative error: 2.532191e-09\n",
      "numerical: 0.810554 analytic: 0.810554, relative error: 1.053018e-09\n",
      "\n",
      "Numerical gradient of W with regularization:\n",
      "numerical: 1.140228 analytic: 1.140228, relative error: 3.219600e-10\n",
      "numerical: 0.823323 analytic: 0.823323, relative error: 5.019232e-10\n",
      "numerical: -0.403689 analytic: -0.403689, relative error: 1.289467e-09\n",
      "numerical: -0.575643 analytic: -0.575643, relative error: 1.390187e-09\n",
      "numerical: 0.038705 analytic: 0.038705, relative error: 2.395013e-10\n",
      "numerical: 0.066343 analytic: 0.066343, relative error: 8.107566e-09\n",
      "numerical: -3.879461 analytic: -3.879461, relative error: 5.891274e-10\n",
      "numerical: 1.312913 analytic: 1.312913, relative error: 9.665662e-10\n",
      "numerical: 0.646337 analytic: 0.646337, relative error: 1.799561e-10\n",
      "numerical: -0.647827 analytic: -0.647827, relative error: 2.467970e-09\n",
      "\n",
      "Numerical gradient of bias:\n",
      "numerical: -0.016014 analytic: -0.016014, relative error: 2.561523e-08\n",
      "numerical: 0.015196 analytic: 0.015196, relative error: 6.619167e-08\n",
      "numerical: -0.004514 analytic: -0.004514, relative error: 1.673768e-07\n",
      "numerical: -0.003803 analytic: -0.003803, relative error: 2.708023e-07\n",
      "numerical: 0.003711 analytic: 0.003711, relative error: 9.080785e-08\n",
      "numerical: -0.016014 analytic: -0.016014, relative error: 2.561523e-08\n",
      "numerical: 0.028458 analytic: 0.028458, relative error: 7.689211e-09\n",
      "numerical: 0.007746 analytic: 0.007746, relative error: 2.053966e-07\n",
      "numerical: -0.016014 analytic: -0.016014, relative error: 2.561523e-08\n",
      "numerical: -0.003803 analytic: -0.003803, relative error: 2.708023e-07\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "W = np.random.randn(c,n) * 0.0001\n",
    "b = np.random.randn(c,1) * 0.0001\n",
    "loss, grads = softmax_loss_naive(W, b, X_dev, Y_dev_enc, 0.0)\n",
    "dW, db = grads[\"dW\"], grads[\"db\"]\n",
    "\n",
    "# We use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs209b.gradient_check import grad_check_sparse\n",
    "f = lambda W: softmax_loss_naive(W, b, X_dev, Y_dev_enc, 0.0)[0]\n",
    "print(\"Numerical gradient of W without regularization:\")\n",
    "grad_numerical = grad_check_sparse(f, W, dW, 10)\n",
    "\n",
    "# do another gradient check with regularization\n",
    "loss, grads = softmax_loss_naive(W, b, X_dev, Y_dev_enc, 5e1)\n",
    "dW, db = grads[\"dW\"], grads[\"db\"]\n",
    "f = lambda W: softmax_loss_naive(W, b, X_dev, Y_dev_enc, 5e1)[0]\n",
    "print(\"\\nNumerical gradient of W with regularization:\")\n",
    "grad_numerical = grad_check_sparse(f, W, dW, 10)\n",
    "\n",
    "# Verify gradient of bias:\n",
    "from cs209b.gradient_check import grad_check_sparse\n",
    "f = lambda b: softmax_loss_naive(W, b, X_dev, Y_dev_enc, 0.0)[0]\n",
    "print(\"\\nNumerical gradient of bias:\")\n",
    "grad_numerical = grad_check_sparse(f, b, db, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chose_idx = np.random.choice(10, size=2)\n",
    "X_dev[np.array([0]), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(W, b, X, Y, num_iterations=100, learning_rate=1e-5, reg=0,\n",
    "             batch_size=100, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes W and b by running a stochastic gradient descent algorithm using mini-batches.\n",
    "\n",
    "    Arguments:\n",
    "    W -- weights, a numpy array of size (c, n)\n",
    "    b -- bias, of size (c, 1)\n",
    "    X -- data of size (m, n)     (m is the number of examples)\n",
    "    Y -- true \"label\" vector with one-hot encoding of size (m, c)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    reg -- (float) regularization strength.\n",
    "    batch_size -- (integer) number of training examples to use at each step.\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "\n",
    "    Returns:\n",
    "    params -- dictionary containing the weights W and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    c = Y.shape[1]\n",
    "    costs = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        chose_idx = np.random.choice(m, size=batch_size) \n",
    "        X_batch = X[chose_idx, :]    # should have shape (batch_size, n)\n",
    "        y_batch = Y[chose_idx, :]     # should have shape (batch_size, c)\n",
    "\n",
    "        #########################################################################\n",
    "        # TODO: Implement stochastic gradient descent.                          #\n",
    "        #########################################################################\n",
    "        _, grads = softmax_loss_vectorized(W, b, X_batch, y_batch, reg)\n",
    "        dW, db = grads[\"dW\"], grads[\"db\"]\n",
    "        W -= learning_rate*dW\n",
    "        b -= learning_rate*db\n",
    "        cost, _ = softmax_loss_vectorized(W, b, X_batch, y_batch, reg)\n",
    "        \n",
    "        #########################################################################\n",
    "        #                          END OF YOUR CODE                             #\n",
    "        #########################################################################\n",
    "\n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "\n",
    "        # Print the cost every 100 training examples\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" % (i, cost))\n",
    "\n",
    "    params = {\"W\": W, \"b\": b}\n",
    "    grads = {\"dW\": dW, \"db\": db}\n",
    "\n",
    "    return params, grads, costs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.814097\n",
      "Cost after iteration 100: 0.802949\n",
      "Cost after iteration 200: 0.187869\n",
      "Cost after iteration 300: 0.113573\n",
      "Computed costs:  [1.8140970955078461, 0.8029494471438278, 0.18786913943140487, 0.11357349037439537]\n",
      "Expected costs:  [2.2500092769842173, 0.6131757237029045, 0.2379333500616387, 0.12624630382907445]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "W = np.random.randn(c, n) * 0.0001\n",
    "b = np.random.randn(c, 1) * 0.0001\n",
    "params, grads, costs = optimize(W, b, X_dev, Y_dev_enc, num_iterations=400, learning_rate=1e-5, reg=0, batch_size=100, print_cost = True)\n",
    "print(\"Computed costs: \", costs)\n",
    "print(\"Expected costs: \", [2.2500092769842173, 0.6131757237029045, 0.2379333500616387, 0.12624630382907445])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(W, b, X):\n",
    "    '''\n",
    "    Infer 0-c encoding labelss using the learned softmax regression parameters (W, b)\n",
    "\n",
    "    Arguments:\n",
    "    W -- weights, a numpy array of size (c, n)\n",
    "    b -- bias, a numpy array of size (c, 1)\n",
    "    X -- data of size (m, n)\n",
    "\n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array containing all predictions for the examples in X with size (m,).\n",
    "    '''\n",
    "    m = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "    Y_prediction = np.zeros((m,))\n",
    "\n",
    "    #############################################################################\n",
    "    # TODO: Compute the scores and return a prediciton.                         #\n",
    "    #############################################################################\n",
    "    lin_sum = np.matmul(W, X.T) + np.tile(b, (1, m))\n",
    "    exp_sum = np.sum(np.exp(lin_sum), axis=0)\n",
    "    exp_sum = exp_sum.reshape((1, exp_sum.shape[0]))\n",
    "    softmax_matrix = np.exp(lin_sum) / np.tile(exp_sum, (c, 1))\n",
    "    Y_prediction = np.argmax(softmax_matrix, axis=0)\n",
    "\n",
    "    #############################################################################\n",
    "    #                          END OF YOUR CODE                                 #\n",
    "    #############################################################################\n",
    "\n",
    "    assert(Y_prediction.shape == (m,))\n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of predicted errors on dev set:  0\n"
     ]
    }
   ],
   "source": [
    "W = params[\"W\"]\n",
    "b = params[\"b\"]\n",
    "y_pred = predict(W, b, X_dev)\n",
    "print(\"Number of predicted errors on dev set: \", np.sum(np.abs(y_pred-y_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.,  3., -1.,  0., -2.],\n",
       "        [ 1.,  3., -1.,  0., -2.],\n",
       "        [ 1.,  3., -1.,  0., -2.],\n",
       "        [ 1.,  3., -1.,  0., -2.]],\n",
       "\n",
       "       [[ 0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.]]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([[1, 3, -1, 0, -2],\n",
    "                [1, 3, -1, 0, -2],\n",
    "                [1, 3, -1, 0, -2],\n",
    "                [1, 3, -1, 0, -2]])\n",
    "zero_arr = np.zeros(arr.shape)\n",
    "stack_arr = np.zeros((2, arr.shape[0], arr.shape[1]))\n",
    "stack_arr[0] = arr\n",
    "stack_arr[1] = zero_arr\n",
    "stack_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 3., 0., 0., 0.],\n",
       "       [1., 3., 0., 0., 0.],\n",
       "       [1., 3., 0., 0., 0.],\n",
       "       [1., 3., 0., 0., 0.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(stack_arr, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

---
title: "| Homework 2: Unsupervised Learning and Clustering"
header-includes:
  - \usepackage{graphicx}
  - \usepackage{enumerate}
  - \usepackage{verbatim}
  - \usepackage{amsmath}
  - \usepackage{subfigure}
  - \usepackage{parskip}
  - \usepackage[notextcomp]{kpfonts}
  - \usepackage{geometry}
  - \usepackage[T1]{fontenc}
  - \usepackage{inconsolata}
  - \usepackage[dvipsnames]{xcolor}
  - \DeclareMathOperator*{\argmin}{argmin}

output:
  pdf_document:
    toc: yes
  html_document:
    highlight: tango
    theme: flatly
    toc: yes
    toc_float: yes
date: "Jan 23, 2018"
subtitle: Harvard CS 109B, Spring 2018
---
\newcommand{\blue}{\textcolor{blue}}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Homework 2 is due February 19, 2018


# "Handy" Algorithms

In this assignment, you will be working with data collected from a motion capture camera system. The system was used to record 12 different users performing 5 distinct hand postures with markers attached to a left-handed glove. A set of markers on the back of the glove was used to establish a local coordinate system for the hand, and 11 additional markers were attached to the thumb and fingers of the glove. Three markers were attached to the thumb with one above the thumbnail and the other two on the knuckles. Finally, 2 markers were attached to each finger with one above the fingernail and the other in the middle of the finger. A total of 36 features were collected resulting from the camera system. Two other variables in the dataset were the ID of the user and the posture that the user made.

The data were partially preprocessed. First, all markers were transformed to the local coordinate system of the record containing them. Second, each transformed marker with a norm greater than 200 millimeters was eliminated. Finally, any record that contained fewer than three markers was removed.

A few issues with the data are worth noting. Based on the manner in which data were captured, it is likely that, for a given record and user, there exists a near duplicate record originating from the same user. Additionally, There are many instances of missing data in the feature set. These instances are denoted with a ? in the dataset. Finally, there is the potential for imbalanced classes, as there is no guarantee that each user and/or posture is represented with equal frequency in the dataset.

The dataset, provided in CSV format, contains 78,095 rows and 38 columns. Each row corresponds to a single instant or frame as recorded by the camera system. The data are represented in the following manner:

`Class - Integer. The hand posture of the given obervation, with 1=Fist (with thumb out), 2=Stop (hand flat), 3=Point1 (point with index finger), 4=Point2 (point with index and middle fingers), 5=Grab (fingers curled as if to grab).`

`User - Integer. The ID of the user that contributed the record.`

`X0, Y0, Z0, X1, Y1, Z1, ..., X11, Y11, Z11 - Real. The x-coordinate, y-coordinate and z-coordinate of the twelve unlabeled marker positions.`

# 1 Missing Data Imputation

The data contain many missing values. Before attempting to perform any statistical procedures, we will need to address the missing data. One way to address missing data is to impute it.

Given the knowledge of how the data was collected, we can hypothesize that there are two ways in which the data might cluster together: by user and by posture. Perhaps the users have significantly different heights and/or hand sizes, resulting in the data generated by each user to be distinct from each other. Or, perhaps the hand postures are sufficiently unique such that the markers on the glove tend to be grouped together by the posture, regardless of who the user is. We will examine these hypotheses to see if either one provides a reasonable way to impute the data.

For this part of the assignment, you will want to use the following libraries:
```{r}
# import libraries
library(dplyr)
library(ggplot2)
library(cluster)
library(mclust)
library(factoextra)
library(NbClust)
library(dbscan)
library(reshape2)
library(devtools)
library(ggfortify)
library(corrplot)

# read in data
data <- read.csv("postures.csv")
```

Write code to impute the missing data values with the mean of their respective feature (column), grouped by both users and postures. That is, you should create two new dataframes: one where the missing values are replaced by the mean of the user’s feature, and one where the missing values are replaced by the mean of the posture’s feature.

Hint: when loaded into R, the raw CSV might list the observations as factors. You will want to change that. One way to convert factors to numeric is to cast the columns of the dataframe like so:

```{r warning=FALSE, message=FALSE}
# convert factors to numeric
for (i in 1:ncol(data)) {
      data[,i] <- as.numeric(as.character(data[,i])) 
}
```

The “dplyr” package might also be useful for data cleaning.

**Helper Function: Col means data imputation **

Hint:  function to impute means for one column, you will want to adapt this for all the necessary columns. 

```{r}    
# function to impute means for one column
impute_column <- function(column) {
  missing_indices <- which(is.na(column))
  column_mean <- mean(as.numeric(column[-missing_indices]))
  column[missing_indices] <- column_mean
  return(column)
}

# function to impute means for one df
impute_means <- function(df){
  for (j in 3:ncol(df)){
    missing_indices_j <- which(is.na(df[,j]))
    if (length(missing_indices_j > 0 )){ # impute only if column has missing values
      if(length(missing_indices_j) != nrow(df)){ # if column has non-NA values
        df[,j] <- impute_column(df[,j])
      }
      else{ # column has all NA values, impute with global df mean
        df[,j] <- mean(colMeans(df[,-c(1,2)]), na.rm = TRUE)
      }
    }
  }
  return(df)
}
```
$\\$    
**For data imputation, you can use this code after you make necessary adjustment above** 

First, create each imputed posture data frame
```{r}
for (posture in unique(data$Class)) {
  df <- data %>% filter(Class == posture)
  df_imputed_means <- impute_means(df)
  assign(paste0("imputed_posture_means", posture), df_imputed_means)
}
```
$\\$
Second,  create each imputed user data frame
```{r}
for (user in unique(data$User)) {
  df <- data %>% filter(User == user)
  df_imputed_means <- impute_means(df)
  assign(paste0("imputed_user_means", user), df_imputed_means)
}
```
$\\$
Then, stack data frames back together (inserting appropriate variable names instead of ellipsis).
```{r}
imputed_posture_all <- rbind(imputed_posture_means1, imputed_posture_means2,
                             imputed_posture_means3, imputed_posture_means4,
                             imputed_posture_means5)
imputed_user_all <- rbind(imputed_user_means0, imputed_user_means1, imputed_user_means2,
                          imputed_user_means4, imputed_user_means5, imputed_user_means6,
                          imputed_user_means7, imputed_user_means8, imputed_user_means9,
                          imputed_user_means10, imputed_user_means11, imputed_user_means12,
                          imputed_user_means13, imputed_user_means14)
```
$\\$
Finally manage your stack and remove clutter (inserting appropriate variable names instead of ellipsis).

```{r}
rm(imputed_posture_means1, imputed_posture_means2, imputed_posture_means3,
   imputed_posture_means4, imputed_posture_means5)

rm(imputed_user_means0, imputed_user_means1, imputed_user_means2, imputed_user_means4,
   imputed_user_means5, imputed_user_means6, imputed_user_means7, imputed_user_means8,
   imputed_user_means9, imputed_user_means10, imputed_user_means11, imputed_user_means12,
   imputed_user_means13, imputed_user_means14)

rm(posture)
rm(user)
rm(df)
rm(df_imputed_means)

unique((imputed_posture_all %>% filter(Class==5))[,38]) # should return 35.67459
```
# 2 Clustering with k-means
Now that we have imputed the missing values, we can investigate our hypotheses by examining how well the data clusters by user and by posture. In the following problem, we will explore a wider choice of options for the number of centroids.

We will first use the k-means algorithm to carry out the clustering.

(a) Using the “kmeans” function in R, run kmeans on the features using 14 centroids (representing the 14 users). Do not run the algorithm on the entire dataset, as the eventual visualization can become unwieldy. Instead, obtain a random sample of 2,000 observations without replacement, and run the algorithm on the sampled values. Set a seed at ’42’ and set the ’nstart’ parameter in the kmeans function to ’46’ to ensure that we can check your results.
Hint: You can take a random sample of the dataframe’s indices using R’s “sample” function:

```{r} 
# scale the features
imputed_user_all[, 3:ncol(imputed_user_all)] <- scale(imputed_user_all[, 3:ncol(imputed_user_all)])
imputed_posture_all[, 3:ncol(imputed_posture_all)] <- scale(imputed_posture_all[, 3:ncol(imputed_posture_all)])

# take a random sample of 2000 observations
set.seed(42)
samp <- sample(x=1:nrow(imputed_user_all), size=2000, replace=F)
user_cluster_target <- imputed_user_all$User[samp]
user_cluster_features <- imputed_user_all[samp, 3:ncol(imputed_user_all)]

# kmeans clustering by user
user.kmeans <- kmeans(user_cluster_features, 14, iter.max=100, nstart=46)
```
     
(b) Use the “fviz-cluster” function to visualize the results of your clustering algorithm (you will probably want to press the “Zoom” button in the plots section of R Studio so that you can see the results on a larger plot). How much of the variance in the data is explained by the first two principal components? Does it look like the data separate into 14 distinct clusters?

```{r}
fviz_cluster(user.kmeans, data=user_cluster_features, main='User K-Means Clustering K=14')

# PCA
user.pca <- prcomp(user_cluster_features, center=T)
user.pc.vars <- cumsum(user.pca$sdev^2/sum(user.pca$sdev^2))
print(paste0("Variance Explained by the first 2 principal components = ", user.pc.vars[2]))

autoplot(user.pca) + geom_density2d() + ggtitle('First 2 PCs of user.pca')
```
\blue{\textbf{ANSWER (2. Clustering with k-means (b)):} \newline
The amount of variance explained by the first 2 principla components is `r user.pc.vars[2]`. The clusters do not seem to be distinct. They have a lot of overlap with each other. 
}

(c) Compare the results from your clustering algorithm to the actual users. Specifically, make a bar plot showing the assigned cluster from kmeans against the actual user of the observation. Have the area of each bar correspond to the the percentage of observations that belong to a given user. Based on this graph, does it look like the data clusters well by user?

Hint: You will probably want to make use the “geom-bar” function in ggplot2 to do this.
```{r}
# calculate the percentage of observations that belong to a given user
user_kmeans_percent <- matrix(rep(0, 14*14), ncol = 14)
for (i in 1:14) {
  cluster_i <- which(user.kmeans$cluster==i) # indices of users clustered as i
  clustered_users <- user_cluster_target[cluster_i] # users clustered as i
  for (j in 1:14) {
    if (j < 4) {
      type <- j-1
    }
    else {
      type <- j
    }
    users_j <- clustered_users[clustered_users==type]
    user_kmeans_percent[i, j] <- length(users_j) / length(cluster_i) * 100
  }
}
# print(rowSums(user_kmeans_percent)) 
rownames(user_kmeans_percent) <- unique(user_cluster_target)
colnames(user_kmeans_percent) <- unique(user_cluster_target)

# plot
df_user_kmeans_percent <- as.data.frame(user_kmeans_percent)
df_user_kmeans_percent$UserId <- unique(user_cluster_target)
df_user_kmeans_percent_melted <- melt(df_user_kmeans_percent, id.var="UserId")
colnames(df_user_kmeans_percent_melted) <- c("UserClustered", "UserLabeled", "value")
ggplot(df_user_kmeans_percent_melted, aes(x = UserClustered, y = value, fill = UserLabeled)) +
  geom_bar(stat = "identity") + xlab("User Clusters") + 
  ylab("% of each labeled class of Users") +
  ggtitle("% of observations that belong to a given user")
```
\blue{\textbf{ANSWER (2. Clustering with k-means (c)):} \newline
Based on the percentage bar plot, only the cluster for user 0, 1, 7, 10, 13 are pure. Other clusters contain multiple labels of users. Therefore, the data does not cluster well by user.
}

(d) Repeat all of the above steps, but group by posture rather than by user. That is, run the kmeans algorithm with 5 centroids instead of 14. Construct the same plots and answer the same questions.

```{r}     
# take a random sample of 2000 observations
set.seed(42)
samp <- sample(x=1:nrow(imputed_posture_all), size=2000, replace=F)
posture_cluster_target <- imputed_posture_all$Class[samp]
posture_cluster_features <- imputed_posture_all[samp, 3:ncol(imputed_posture_all)]

# kmeans clustering by posture
posture.kmeans <- kmeans(posture_cluster_features, 5, iter.max=20, nstart=46)
fviz_cluster(posture.kmeans, data=posture_cluster_features, 
             main='Posture K-Means Clustering K=5')

# PCA
posture.pca <- prcomp(posture_cluster_features, center=T)
posture.pc.vars <- cumsum(posture.pca$sdev^2/sum(posture.pca$sdev^2))
print(paste0("Variance Explained by the first 2 principal components = ", posture.pc.vars[2]))

autoplot(posture.pca) + geom_density2d() + ggtitle('First 2 PCs of posture.pca')


# calculate the percentage of observations that belong to a given posture
posture_kmeans_percent <- matrix(rep(0, 5*5), ncol = 5)
for (i in 1:5) {
  cluster_i <- which(posture.kmeans$cluster==i) # indices of postures clustered as i
  clustered_postures <- posture_cluster_target[cluster_i] # postures clustered as i
  for (j in 1:5) {
    type <- j
    postures_j <- clustered_postures[clustered_postures==type]
    posture_kmeans_percent[i, j] <- length(postures_j) / length(cluster_i) * 100
  }
}
# print(rowSums(posture_kmeans_percent))
rownames(posture_kmeans_percent) <- unique(posture_cluster_target)
colnames(posture_kmeans_percent) <- unique(posture_cluster_target)

# plot
df_posture_kmeans_percent <- as.data.frame(posture_kmeans_percent)
df_posture_kmeans_percent$PostureId <- unique(posture_cluster_target)
df_posture_kmeans_percent_melted <- melt(df_posture_kmeans_percent, id.var="PostureId")
colnames(df_posture_kmeans_percent_melted) <- c("PostureClustered", "PostureLabeled", "value")
ggplot(df_posture_kmeans_percent_melted, 
       aes(x = PostureClustered, y = value, fill = PostureLabeled)) +
  geom_bar(stat = "identity") + xlab("Posture Clusters") + 
  ylab("% of each labeled class of Postures") + 
  ggtitle("% of observations that belong to a given posture")
```
\blue{\textbf{ANSWER (2. Clustering with k-means (d)):} \newline
The amount of variance explained by the first 2 principla components is `r posture.pc.vars[2]`. The clusters seem to be distinct. 
\newline
\newline
Based on the percentage bar plot, all clusters except for cluster 5 are mostly pure. Therefore, we can say that the data clusters well by user.
}

(e) What do the results of the bar plot clustered by posture suggest about the data? Why does this make sense in the context of what we know about the problem?
\blue{\textbf{ANSWER (2. Clustering with k-means (e)):} \newline
The bar plot clustered by posture shows that 1) there are 2 different clusters (2 and 4) that have the same posture, while 2) there's another cluster (5) that consists of 2 distinct postures. 
\newline
\newline
Based on the posture classes 1=Fist (thumb out),
2=Stop (hand flat), 3=Point1 (point with index finger), 4=Point2 (point with index and
middle fingers), and 5=Grab (fingers curled as if to grab), the hand flat posture (label 2) could be different depending on whether they fingers together or spread. Pointing with index finger only (label 3) and pointing with both index and middle fingers (label 4) are similar postures, and thus they do not form distinct clusters. Thumb out (label 1) being well clustered could be explained by having one more markers on the thumb than on other fingers. Grab (label 5) being well clustered could be explained by it being very different posture compared to the other posture types.
}

(f) Using all of the information gleaned from this problem, how do you recommend the missing data be imputed? Why?
\blue{\textbf{ANSWER (2. Clustering with k-means (f)):} \newline
Since 1) the portion of explained variance of the imputed data by posture is higher than that of the imputed data by user, and 2) the percentage bar plot shows that "by postures" can better cluster the data than "by user", we would recommend the missing data be imputed by posture.
}

# 3 Clustering Evaluation
In the previous problem, we used k-means with 5 and 14 centroids to decide how we should impute missing data. In this problem, we will investigate various ways of evaluating the quality of a clustering assignment.

(a) Use the elbow method to evaluate the best choice of the number of clusters, plotting the total within-cluster variation against the number of clusters for k-means clustering with k $\in$  (1, 2, ... 15).

```{r}
# elbow - method = "wss"
fviz_nbclust(x = posture_cluster_features, FUNcluster = kmeans, 
             method = "wss", k.max = 15, nstart=46) +
  ggtitle("Elbow method: optimal K for K-means clustering by posture") +
  geom_vline(xintercept=4,linetype=2)
```

(b) Use the average silhouette to evaluate the choice of the number of clusters for k-means clustering with k $\in$  (1, 2, ... 15). Plot the results.

```{r}
# average silhouette - method = "silhouette"
fviz_nbclust(x = posture_cluster_features, FUNcluster = kmeans, 
             method = "silhouette", k.max = 15, nstart=46) +
  ggtitle("Silhouette method: optimal K for K-means clustering by posture") 
```

(c) Use the gap statistic to evaluate the choice of the number of clusters for k-means clustering with k $\in$  (1, 2, ... 15). Plot the results. Be patient – this might take a few minutes.

```{r}
gapstat = clusGap(posture_cluster_features, FUN=kmeans, iter.max=30, nstart=46, d.power=2, K.max=15, B=20)
print(gapstat, method="Tibs2001SEmax")
fviz_gap_stat(gapstat, 
  maxSE=list(method="Tibs2001SEmax", SE.factor=1)) + 
  ggtitle("Gap statistic: optimal K for K-means clustering by posture") 
```

(d) After analyzing the plots produced by all three of these measures, discuss the number of clusters that you feel is the best fit for this dataset. Defend your answer with evidence from the previous parts of this assignment, the three graphs produced here, and what you surmise about this dataset.

\blue{\textbf{ANSWER (3. Clustering evaluation (d)):} \newline
By previous parts of analysis, especially the percentage bar plot by user and posture, we found that posture can better cluster the data than user can. However, elbow method picks an optimal K to be 4; average silhouette picks an optimal to be 2 while the gap statistic picks an optimal K to be 14. 
(Number of clusters????)
}


# 4 Other Clustering Algorithms
Up until now, we have used the k-means algorithm to cluster the data. In this problem, we will explore other methods used to create clusters.

(a) Hierarchical clustering: Implement agglomerative clustering (using Ward’s method) and divisive clustering. Plot the results of these algorithms using a dendrogram and interpret the results.
Hint: Use the “agnes” and “diana” functions, respectively.

```{r}
# agglomerative clustering (using Ward’s method)
user.agnes <- agnes(user_cluster_features, method="ward")
pltree(user.agnes, cex=0.5, hang= -1,
  main="AGNES fit (Ward's method) of User data",
  xlab="User data",sub="")
rect.hclust(user.agnes, k=14, border=2:5)

# divisive clustering - diana
user.diana = diana(user_cluster_features)
pltree(user.diana, cex=0.5, hang= -1,
  main="DIANA fit of User data", xlab="User data", sub="")
rect.hclust(user.diana, k=14, border=2:5)
```
\blue{\textbf{ANSWER (4. Other Clustering Algorithms (a)):} \newline
In a dendrogram, the height of a cluster represents the largest diameter of the cluster, i.e., the largest dissimilarity between two members points within this cluster. We found that the maximum height of aggolomerative clustering is larger than the maximum height of divisive clustering. The resulting 14 clusters from aggolomerative clustering have more dissimilar heights than the from divisive clustering. This is because aggolomerative clustering is greedy picking the most similar pair first with individual data points and then with larger groups, while devisive clustering is greedy with first the entire group and then smaller ones. 
(Interprete results - Agnes VS. Diana ????)
}


```{r}
user.grp.agnes <- cutree(user.agnes, k=14)
fviz_cluster(list(data=user_cluster_features, cluster=user.grp.agnes))

user.grp.diana <- cutree(user.diana, k=14)
fviz_cluster(list(data=user_cluster_features, cluster=user.grp.diana))
```


(b) Soft clustering: Run fuzzy clustering and a Gaussian mixture model on the scaled features. For the fuzzy clustering, run the algorithm with 5 and 14 clusters and plot the results using the “fviz-silhouette” function. For the Gaussian mixture model, the “Mclust” algorithm chooses the optimal number of clusters internally; report the number of clusters it selects.  Also display the membership probabilities for the first 10 observations in your sample.

Hint: Use the “fanny” and “Mclust” functions, respectively. You might need to adjust the “memb.exp” parameter to something between 1 and 2 to get the function to run correctly. Make sure to include analysis for trial-and-error of fanny parameters. Justify your results.

```{r}
# fuzzy clustering - 5 clusters
param <- seq(1.1, 2, 0.1)
for (i in param){
  print(i)
  user.fanny.k5 = fanny(user_cluster_features, k=5, memb.exp = i)
}

user.fanny.k5.opt = fanny(user_cluster_features, k=5, memb.exp = 1.1)
fviz_silhouette(silhouette(user.fanny.k5.opt),
    main=paste0("Silhouette plot for FUZZY clustering - 5 clusters, optimal memb.exp = ", 1.1))

# the membership probabilities for the first 10 observations in the sample
print(head(round(user.fanny.k5.opt$membership,3), 10)) 
corrplot(user.fanny.k5.opt$membership[1:10,], is.corr=F)
```

```{r}
# fuzzy clustering - 14 clusters
param <- seq(1.1, 2, 0.1)
for (i in param){
  print(i)
  user.fanny.k14 = fanny(user_cluster_features, k=14, memb.exp = i)
}

user.fanny.k14.opt = fanny(user_cluster_features, k=14, memb.exp = 1.1)
fviz_silhouette(silhouette(user.fanny.k14.opt),
    main=paste0("Silhouette plot for FUZZY clustering - 14 clusters, optimal memb.exp = ", 1.1))

# the membership probabilities for the first 10 observations in the sample
print(head(round(user.fanny.k14.opt$membership,3), 10)) 
corrplot(user.fanny.k14.opt$membership[1:10,], is.corr=F)
```


```{r}
# Gaussian mixture model on the scaled features
user.mc = Mclust(user_cluster_features)
print(summary(user.mc))

# optimal number of clusters
print(paste0("Optimal number of clusters by Mclust = ", user.mc$G))

# the membership probabilities for the first 10 observations in the sample
print(head(round(user.mc$z, 3), 10))
fviz_cluster(user.mc, ellipse.type='norm', geom="point") +
  ggtitle("Gaussian Mixture Model Clustering")
corrplot(user.mc$z[1:10,], is.corr=F)
```

\blue{\textbf{ANSWER (4. Other Clustering Algorithms (b)):} \newline
To find the optimal `memb.exp`, we tried values between 1.1 and 2 (step by 0.1). The only value that did not give the error "the memberships are all very close to 1/k" was 1.1. The `memb.exp` parameter indicates membership ambiguity between clusters. Values larger than 1.1 appear to be too fuzzy to pick out the largest proabaility of belonging to certain group.
\newline
\newline
The optimal number of clusters by Mclust is `r user.mc$G`. The membership probability of the Gaussian mixture model shows thaat the model has very high confidence in the clustering results.
}

(c) (AC 209b students only) Density-based clustering: Apply DBSCAN to the data. Determine a reasonable combination of $\epsilon$, the radius of the neighborhood around an observation, and the number of nearest neighbors within the $\epsilon$-neighborhood to be considered a core point. You should construct a knee plot to determine the choice of $\epsilon$. Summarize the results using a principal components plot, and comment on the clusters and outliers identified. How does the clustering produced by DBSCAN compare to the previous methods?
Read Section 2.2 of the R vignette on DBSCAN

https://cran.r-project.org/web/packages/dbscan/vignettes/dbscan.pdf to learn about the OPTICS algorithm.

```{r}
# knee plot
kNNdistplot(scale(user_cluster_features))
abline(7, 0, lty=2, lwd=2, col="red")
```


```{r}
user.db = dbscan(user_cluster_features, eps=7, minPts=5)
fviz_cluster(user.db, user_cluster_features, ellipse=F, geom = "point") +
  ggtitle("DBSCAN clustering")
```
\blue{\textbf{ANSWER (4. Other Clustering Algorithms (209 Question)} \newline
Based on the knee plot, we selected $\epsilon = 7$. Density-based clustering gives one cluster. It gives very different clustering result compared to the previous clustering algorithms. K-means and hierarchical clustering find clusters by minimizing data variance within pre-specified number of clusters. Gaussian mixture model assumes the data comes from some distribution of convex shapes. By contrast, density-based clustering does not assume parametric distributions or number of clusters before clustering. It is more capable of finding arbitrarily-shaped clusters without prior knowledge of the number of clusters. 
}

1. Describe the difference in goal between the DBSCAN and OPTICS algorithm. You may need to refer to the references cited within.
\blue{\textbf{ANSWER (4. Other Clustering Algorithms (209 Question) - 1.:} \newline
DBSCAN has only one value for the $\epsilon$ parameter, which is a measure of density neighborhood size. It detects clusters consisting of data points that are at most $\epsilon$ distant. With the single valued neighborhood size, DBSCAN cannot find clusters of varying densities. 
\newline
\newline
By contrast, OPTICS's $\epsilon$ does not represent a single value. Instead, it is the upper threshold for different $\epsilon$'s applied to different clusters. Therefore, OPTICS can detect clusters of varying densities. By exploring neighbors of each point along its core- and reachability-distance (from lowest to highest), OPTICS outputs a sequence of data points with their reachability-distances. Low reachability-distances shown as valleys represent clusters separated by peaks represent- ing points with larger distances.
}

2. Run the OPTICS algorithm on the data within the dbscan package. Choose (and justify) an appropriate value of $\epsilon$ and the minimum number of points in the $\epsilon$-neighborhood. Interpret the results of the clustering.

Hint: Make sure to also use and plot `extractXi()` with parameter `xi=0.5` to properly visualize your results. See documentation suggested above.

```{r}
user.op <- optics(user_cluster_features, eps = 10, minPts = 10)
plot(user.op)
user.opXi <- extractXi(user.op, xi = 0.05)
user.opXi # result of extracted clusters from optics
plot(user.opXi)
user.opXi$clusters_xi # cluster id's
hullplot(scale(user_cluster_features), user.opXi, main = "OPTICS - extractXi")
```
\blue{\textbf{ANSWER (4. Other Clustering Algorithms (209 Question) - 2.:} \newline
OPTICS gives 6 clusters. Based on the clusters visualization, data points that do not cluster on the principle component plot could be denstiy reacheable/connected. This suggests that the largest part of the data variance could be explained by posture difference. At the same time, different users may have different habits of giving the same postures, there is additional data variance caused by differenct users.
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AC 209B: Homework 6\n",
    "### Neural Style Transfer\n",
    "** Harvard University ** <br>\n",
    "** Spring  2018 ** <br>\n",
    "** Instructors:** Pavlos Protopapas and Mark Glickman \n",
    "\n",
    "---\n",
    "\n",
    "### INSTRUCTIONS\n",
    "\n",
    "- To submit your assignment follow the instructions given in canvas.\n",
    "- Make sure the homework runs correctly before you submit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Your partner's name (if you submit separately): **\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "We will implement the neural style transfer technique presented in [\"Image Style Transfer Using Convolutional Neural Networks\" (Gatys et al., CVPR 2015)](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf). As we have seen in the a-section, this technique combines the content and style of two given images, and generates a third one, reflecting the elements of the original images.\n",
    "\n",
    "The purpose of this homework will be to understand and program the loss functions that mimics the contents of an image, the style, and combines them in a general loss function together with some regularizer.\n",
    "\n",
    "We recommend you to read the lecture notes in order to implement the following exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Run the following code to load the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.applications import vgg16, vgg19\n",
    "from keras.preprocessing.image import load_img\n",
    "\n",
    "from scipy.misc import imsave\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "\n",
    "# preprocessing\n",
    "from utils import preprocess_image, deprocess_image\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Content loss (1 pt)\n",
    "We can generate an image that combines the content of one image with the style of another with a loss function that incorporates this information. This is achieved with two terms, one that mimics the specific ativations of a certain layer for the content image, and a second term that mimics the style. The variable to optimize in the loss function will be a generated image that aims to minimize the proposed cost. Note that to optimize this function, we will perform gradient descent __on the pixel values__, rather than on the neural network weights.\n",
    "\n",
    "We will load a trained neural network called VGG-16 proposed in [1](https://arxiv.org/pdf/1409.1556.pdf), who secured the first and second place in the localisation and classification tracks of ImageNet Challenge in 2014, respectively. This network has been trained to discriminate over 1000 classes over more than a million images. We will use the activation values obtained for an image of interest to represent the content and styles. In order to do so, we will feed-forward the image of interest and observe it's activation values at the indicated layer.\n",
    "\n",
    "The content loss function measures how much the feature map of the generated image differs from the feature map of the source image. We will only consider a single layer to represent the contents of an image. The authors of this technique indicated they obtained better results when doing so. We denote the feature maps for layer $l$ with $a^{[l]} \\in \\mathbb{R}^{n_H^{[l]} \\times n_W^{[l]} \\times n_C^{[l]}}$. Parameter $n_C^{[l]}$ is the number of filters/channels in layer $l$, $n_H^{[l]}$ and $n_W^{[l]}$ are the height and width.\n",
    "\n",
    "The content loss is then given by:\n",
    "\\begin{equation}\n",
    "    J^{[l]}_C = \\big\\Vert a^{[l](G)} - a^{[l](C)} \\big\\Vert^2_{\\mathcal{F}},\n",
    "\\end{equation}\n",
    "where $a^{[l](G)}$ refers to the layer's activation values of the generated image, and $a^{[l](C)}$ to those of the content image.\n",
    "\n",
    "** Implement funtion `feature_reconstruction_loss` that computes the loss of two feature inputs. You will need to use [keras backend functions](https://keras.io/backend/#backend-functions) to complete the exercise. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_reconstruction_loss(base, output):\n",
    "    \"\"\"\n",
    "    Compute the content loss for style transfer.\n",
    "    \n",
    "    Inputs:\n",
    "    - output: features of the generated image, Tensor with shape [height, width, channels]\n",
    "    - base: features of the content image, Tensor with shape [height, width, channels]\n",
    "    \n",
    "    Returns:\n",
    "    - scalar content loss\n",
    "    \"\"\"\n",
    "    return K.sum(K.square(output - base))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result:           605.6219\n",
      "Expected result:  605.62195\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "base = np.random.randn(10,10,3)\n",
    "output = np.random.randn(10,10,3)\n",
    "a = K.constant(base)\n",
    "b = K.constant(output)\n",
    "test = feature_reconstruction_loss(a, b)\n",
    "print('Result:          ', K.eval(test))\n",
    "print('Expected result: ', 605.62195)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Style loss: computing the Gram matrix (2 pts)\n",
    "\n",
    "The style measures the similarity among filters in a set of layers. In order to compute that similarity, we will compute the Gram matrix of the activation values for the style layers, i.e., $a^{[l]}$ for some set $\\mathcal{L}$. The Gram matrix is related to the empirical covariance matrix, and therefore, reflects the statistics of the activation values.\n",
    "\n",
    "Given a feature map $a^{[l]}$ of shape $(n_H^{[l]}, n_W^{[l]}, n_C^{[l]})$, the Gram matrix has shape $(n_C^{[l]}, n_C^{[l]})$ and its elements are given by:\n",
    "\\begin{equation*}\n",
    "    G^{[l]}_{k k'} = \\sum_{i=1}^{n_H^{[l]}} \\sum_{j=1}^{n_W^{[l]}} a^{[l]}_{ijk} a^{[l]}_{ijk'}.\n",
    "\\end{equation*}\n",
    "The output is a 2-D matrix which approximately measures the cross-correlation among different filters for a given layer. This in essence constitutes the style of a layer.\n",
    "\n",
    "** Implement a function that computes the Gram matrix of a given keras tensor. To receive full credit, __do not use any loops__. This can be accomplished efficiently if $x$ is reshaped as a tensor of shape ($n_C^{[l]} \\times n_H^{[l]} n_W^{[l]}$). You will need to use [keras backend functions](https://keras.io/backend/#backend-functions) to complete the exercise. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    \"\"\"\n",
    "    Computes the outer-product of the input tensor x.\n",
    "\n",
    "    Input:\n",
    "    - x: input tensor of shape (H, W, C)\n",
    "\n",
    "    Returns:\n",
    "    - tensor of shape (C, C) corresponding to the Gram matrix of\n",
    "    the input image.\n",
    "    \"\"\"\n",
    "    # reshape to (C=2, H=0, W=1) and flatten\n",
    "    features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1))) \n",
    "    return K.dot(features, K.transpose(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result:\n",
      " [[99.757225  -9.961859  -1.4740529]\n",
      " [-9.961859  86.854355  -4.1411047]\n",
      " [-1.4740529 -4.1411047 82.30109  ]]\n",
      "Expected:\n",
      " [[99.75723   -9.96186   -1.4740534]\n",
      " [-9.96186   86.854324  -4.141108 ]\n",
      " [-1.4740534 -4.141108  82.30106  ]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x_np = np.random.randn(10,10,3)\n",
    "x = K.constant(x_np)\n",
    "test = gram_matrix(x)\n",
    "print('Result:\\n', K.eval(test))\n",
    "print('Expected:\\n', np.array([[99.75723, -9.96186, -1.4740534], [-9.96186, 86.854324, -4.141108 ], [-1.4740534, -4.141108, 82.30106  ]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Style loss: layer's loss (2 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can tackle the style loss. For a given layer $l$, the style loss is defined as follows:\n",
    "\\begin{equation*}\n",
    "    J^{[l]}_S = \\frac{1}{4 (n^{[l]}_W n^{[l]}_H)^2} \\Big\\Vert G^{[l](S)} - G^{[l](G)}\\Big\\Vert^2_{\\mathcal{F}}.\n",
    "\\end{equation*}\n",
    "\n",
    "In practice we compute the style loss at a set of layers $\\mathcal{L}$ rather than just a single layer $l$; then the total style loss is the sum of style losses at each layer:\n",
    "\n",
    "$$J_S = \\sum_{l \\in \\mathcal{L}} \\lambda_l J^{[l]}_S$$\n",
    "where $\\lambda_l$ corresponds to a weighting parameter. You do not need to implement the weighted sum, this is given to you and coded in the main function of this implmentation.\n",
    "\n",
    "** Implement `style_reconstruction_loss` that computes the loss for a given layer $l$. To receive full credit, __do not use any loops__. You will need to use [keras backend functions](https://keras.io/backend/#backend-functions) to complete the exercise. ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_reconstruction_loss(base, output):\n",
    "    \"\"\"\n",
    "    Computes the style reconstruction loss. It encourages the output img \n",
    "    to have same stylistic features as style image.\n",
    "    \n",
    "    Inputs:\n",
    "    - base: features at given layer of the style image.\n",
    "    - output: features of the same length as base of the generated image.\n",
    "      \n",
    "    Returns:\n",
    "    - style_loss: scalar style loss\n",
    "    \"\"\"\n",
    "    H, W, C = [int(x) for x in base.shape]\n",
    "    gram_base = gram_matrix(base)\n",
    "    gram_output = gram_matrix(output)\n",
    "    factor = 1/ (4* (H*W)**2)\n",
    "    loss = factor * K.sum(K.square(gram_output - gram_base))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result:   0.09799156\n",
      "Expected: 0.09799164\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(10,10,3)\n",
    "y = np.random.randn(10,10,3)\n",
    "a = K.constant(x)\n",
    "b = K.constant(y)\n",
    "test = style_reconstruction_loss(a, b)\n",
    "print('Result:  ', K.eval(test))\n",
    "print('Expected:', 0.09799164)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D: Total-variation regularization (2 pts)\n",
    "We will also encourage smoothness in the image using a total-variation regularizer. This penalty term will reduce variation among the neighboring pixel values.\n",
    "\n",
    "The following expression constitues the regularization penalty over all pairs that are next to each other horizontally or vertically. The expression is independent among different RGB channels.\n",
    "\\begin{equation*}\n",
    "    J_{tv} = \\sum_{c=1}^3\\sum_{i=1}^{n^{[l]}_H-1} \\sum_{j=1}^{n^{[l]}_W-1} \\left( (x_{i,j+1, c} - x_{i,j,c})^2 + (x_{i+1, j,c} - x_{i,j,c})^2  \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "** In the next cell, fill in the definition for the TV loss term. To receive full credit, __your implementation should not have any loops__. **\n",
    "\n",
    "__Remark:__ in this exercice $x$ has dimension $(1, n_H^{[l]}, n_W^{[l]}, n_C^{[l]})$, which is different from the 3D-tensors we used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variation_loss(x):\n",
    "    \"\"\"\n",
    "    Total variational loss. Encourages spatial smoothness \n",
    "    in the output image.\n",
    "    \n",
    "    Inputs:\n",
    "    - x: image with pixels, has shape 1 x H x W x C.\n",
    "      \n",
    "    Returns:\n",
    "    - total variation loss, a scalar number.\n",
    "    \"\"\"\n",
    "    I, H, W, C = [int(j) for j in x.shape]\n",
    "    a = K.square(x[:, :H-1, :W-1, :] - x[:, 1:, :W-1, :])\n",
    "    b = K.square(x[:, :H-1, :W-1, :] - x[:, :H-1, 1:, :])\n",
    "\n",
    "    return K.sum(a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result:   937.0538\n",
      "Expected: 937.0538\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x_np = np.random.randn(1,10,10,3)\n",
    "x = K.constant(x_np)\n",
    "test = total_variation_loss(x)\n",
    "print('Result:  ', K.eval(test))\n",
    "print('Expected:', 937.0538)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part E: Style transfer (2 pts)\n",
    "We now put it all together and generate some images! The `style_transfer` function below combines all the losses you coded up above and optimizes for an image that minimizes the total loss. Read the code and comments to understand the procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_transfer(base_img_path, style_img_path, output_img_path, convnet='vgg16', \n",
    "        content_weight=3e-2, style_weights=(20000, 500, 12, 1, 1), tv_weight=5e-2, content_layer='block4_conv2', \n",
    "        style_layers=['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1'], iterations=50):\n",
    "    \n",
    "    print('\\nInitializing Neural Style model...')\n",
    "\n",
    "    # Determine the image sizes. Fix the output size from the content image.\n",
    "    print('\\n\\tResizing images...')\n",
    "    width, height = load_img(base_img_path).size\n",
    "    new_dims = (height, width)\n",
    "\n",
    "    # Preprocess content and style images. Resizes the style image if needed.\n",
    "    content_img = K.variable(preprocess_image(base_img_path, new_dims))\n",
    "    style_img = K.variable(preprocess_image(style_img_path, new_dims))\n",
    "\n",
    "    # Create an output placeholder with desired shape.\n",
    "    # It will correspond to the generated image after minimizing the loss function.\n",
    "    output_img = K.placeholder((1, height, width, 3))\n",
    "    \n",
    "    # Sanity check on dimensions\n",
    "    print(\"\\tSize of content image is: {}\".format(K.int_shape(content_img)))\n",
    "    print(\"\\tSize of style image is: {}\".format(K.int_shape(style_img)))\n",
    "    print(\"\\tSize of output image is: {}\".format(K.int_shape(output_img)))\n",
    "\n",
    "    # Combine the 3 images into a single Keras tensor, for ease of manipulation\n",
    "    # The first dimension of a tensor identifies the example/input.\n",
    "    input_img = K.concatenate([content_img, style_img, output_img], axis=0)\n",
    "\n",
    "    # Initialize the vgg16 model\n",
    "    print('\\tLoading {} model'.format(convnet.upper()))\n",
    "\n",
    "    if convnet == 'vgg16':\n",
    "        model = vgg16.VGG16(input_tensor=input_img, weights='imagenet', include_top=False)\n",
    "    else:\n",
    "        model = vgg19.VGG19(input_tensor=input_img, weights='imagenet', include_top=False)\n",
    "        \n",
    "    print('\\tComputing losses...')\n",
    "    # Get the symbolic outputs of each \"key\" layer (they have unique names).\n",
    "    # The dictionary outputs an evaluation when the model is fed an input.\n",
    "    outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
    "\n",
    "    # Extract features from the content layer\n",
    "    content_features = outputs_dict[content_layer]\n",
    "\n",
    "    # Extract the activations of the base image and the output image\n",
    "    base_image_features = content_features[0, :, :, :]  # 0 corresponds to base\n",
    "    combination_features = content_features[2, :, :, :] # 2 coresponds to output\n",
    "\n",
    "    # Calculate the feature reconstruction loss\n",
    "    content_loss = content_weight * feature_reconstruction_loss(base_image_features, combination_features)\n",
    "\n",
    "    # For each style layer compute style loss\n",
    "    # The total style loss is the weighted sum of those losses\n",
    "    temp_style_loss = K.variable(0.0)       # we update this variable in the loop\n",
    "    weight = 1.0 / float(len(style_layers))\n",
    "    \n",
    "    for i, layer in enumerate(style_layers):\n",
    "        # extract features of given layer\n",
    "        style_features = outputs_dict[layer]\n",
    "        # from those features, extract style and output activations\n",
    "        style_image_features = style_features[1, :, :, :]   # 1 corresponds to style image\n",
    "        output_style_features = style_features[2, :, :, :]  # 2 coresponds to generated image\n",
    "        temp_style_loss += style_weights[i] * weight * \\\n",
    "                    style_reconstruction_loss(style_image_features, output_style_features)\n",
    "    style_loss = temp_style_loss\n",
    "\n",
    "    # Compute total variational loss.\n",
    "    tv_loss = tv_weight * total_variation_loss(output_img)\n",
    "\n",
    "    # Composite loss\n",
    "    total_loss = content_loss + style_loss + tv_loss\n",
    "    \n",
    "    # Compute gradients of output img with respect to total_loss\n",
    "    print('\\tComputing gradients...')\n",
    "    grads = K.gradients(total_loss, output_img)\n",
    "    \n",
    "    outputs = [total_loss] + grads\n",
    "    loss_and_grads = K.function([output_img], outputs)  \n",
    "    \n",
    "    # Initialize the generated image from random noise\n",
    "    x = np.random.uniform(0, 255, (1, height, width, 3)) - 128.\n",
    "    \n",
    "    # Loss function that takes a vectorized input image, for the solver\n",
    "    def loss(x):\n",
    "        x = x.reshape((1, height, width, 3))   # reshape\n",
    "        return loss_and_grads([x])[0]\n",
    "    \n",
    "    # Gradient function that takes a vectorized input image, for the solver\n",
    "    def grads(x):\n",
    "        x = x.reshape((1, height, width, 3))   # reshape\n",
    "        return loss_and_grads([x])[1].flatten().astype('float64')\n",
    "    \n",
    "    # Fit over the total iterations\n",
    "    for i in range(iterations):\n",
    "        print('\\n\\tIteration: {}'.format(i+1))\n",
    "\n",
    "        toc = time.time()\n",
    "        x, min_val, info = fmin_l_bfgs_b(loss, x.flatten(), fprime=grads, maxfun=20)\n",
    "\n",
    "        # save current generated image\n",
    "        img = deprocess_image(x.copy(), height, width)\n",
    "        fname = output_img_path + '_at_iteration_%d.png' % (i+1)\n",
    "        imsave(fname, img)\n",
    "\n",
    "        tic = time.time()\n",
    "\n",
    "        print('\\t\\tImage saved as', fname)\n",
    "        print('\\t\\tLoss: {:.2e}, Time: {} seconds'.format(float(min_val), float(tic-toc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = vgg16.VGG16(weights='imagenet', include_top=False)\n",
    "m1_layers = [layer.name for layer in m1.layers]\n",
    "m1_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = vgg19.VGG19(weights='imagenet', include_top=False)\n",
    "m2_layers = [layer.name for layer in m2.layers]\n",
    "m2_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate pictures\n",
    "\n",
    "** Try `style_transfer` on the three different parameter sets below.**  Feel free to add your own, and make sure to include the results of the style transfer in your submitted notebook. You may adjust any parameter you feel can improve your result.\n",
    "\n",
    "* The `base_img_path` is the filename of content image.\n",
    "* The `style_img_path` is the filename of style image.\n",
    "* The `output_img_path` is the filename of generated image.\n",
    "* The `convnet` is for the neural network weights, VGG-16 or VGG-19.\n",
    "* The `content_layer` specifies which layer to use for content loss.\n",
    "* The `content_weight` weights the content loss in the overall composite loss function. Increasing the value of this parameter will make the final image look more realistic (closer to the original content).\n",
    "* `style_layers` specifies a list of which layers to use for the style loss. \n",
    "* `style_weights` specifies a list of weights to use for each layer in style_layers (each of which will contribute a term to the overall style loss). We generally use higher weights for the earlier style layers because they describe more local/smaller scale features, which are more important to texture than features over larger receptive fields. In general, increasing these weights will make the resulting image look less like the original content and more distorted towards the appearance of the style image.\n",
    "* `tv_weight` specifies the weighting of total variation regularization in the overall loss function. Increasing this value makes the resulting image look smoother and less jagged, at the cost of lower fidelity to style and content. \n",
    "\n",
    "__ Submit the best created images for each three content-style pairs. __ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great wave of Kanagawa + Chicago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "'base_img_path' : 'images/inputs/chicago.jpg', \n",
    "'style_img_path' : 'images/inputs/great_wave_of_kanagawa.jpg', \n",
    "'output_img_path' : 'images/results/wave_chicago', \n",
    "'convnet' : 'vgg16', \n",
    "'content_weight' : 500, \n",
    "'style_weights' : (20, 20, 30, 10, 10),\n",
    "'tv_weight' : 200, \n",
    "'content_layer' : 'block4_conv2', \n",
    "'style_layers' : ['block1_conv1',\n",
    "                  'block2_conv1',\n",
    "                  'block3_conv1', \n",
    "                  'block4_conv1', \n",
    "                  'block5_conv1'], \n",
    "'iterations' : 50\n",
    "}\n",
    "\n",
    "style_transfer(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Kanagawa_Chicago](images/results/wave_chicago_at_iteration_50.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starry night + TÃ¼bingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "'base_img_path' : 'images/inputs/tubingen.jpg', \n",
    "'style_img_path' : 'images/inputs/starry_night.jpg', \n",
    "'output_img_path' : 'images/results/starry_tubingen', \n",
    "'convnet' : 'vgg16', \n",
    "'content_weight' : 100, \n",
    "'style_weights' : (1000, 100, 12, 1, 1),\n",
    "'tv_weight' : 200, \n",
    "'content_layer' : 'block4_conv2', \n",
    "'style_layers' : ['block1_conv1',\n",
    "                  'block2_conv1',\n",
    "                  'block3_conv1', \n",
    "                  'block4_conv1', \n",
    "                  'block5_conv1'], \n",
    "'iterations' : 50\n",
    "}\n",
    "\n",
    "style_transfer(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![starry_tubingen](images/results/starry_tubingen_50.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Portrait of a man (El Greco) + Pavlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "'base_img_path' : 'images/inputs/pavlos_protopapas.jpg', \n",
    "'style_img_path' : 'images/inputs/portrait_of_a_man.jpg', \n",
    "'output_img_path' : 'images/results/portrait_of_pavlos', \n",
    "'convnet' : 'vgg16', \n",
    "'content_weight' : 1000, \n",
    "'style_weights' : (500, 100, 12, 1, 1),\n",
    "'tv_weight' : 200, \n",
    "'content_layer' : 'block4_conv2', \n",
    "'style_layers' : ['block1_conv1',\n",
    "                  'block2_conv1',\n",
    "                  'block3_conv1', \n",
    "                  'block4_conv1', \n",
    "                  'block5_conv1'], \n",
    "'iterations' : 50\n",
    "}\n",
    "\n",
    "style_transfer(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![portrait_pavlos](images/results/portrait_of_pavlos_50.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part F: Pavlos Grandmother (1 pt)\n",
    "\n",
    "Use the image of Pavlos' grandmother `images/inputs/pavlos_grandmother.jpg` and an artistic style of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "'base_img_path' : 'images/inputs/resized_pavlos_grandma.jpg', \n",
    "'style_img_path' : 'images/inputs/woman-with-hat-matisse.jpg', \n",
    "'output_img_path' : 'images/results/pavlos_grandma_woman_with_hat', \n",
    "'convnet' : 'vgg19', \n",
    "'content_weight' : 1000, \n",
    "'style_weights' : (500, 100, 12, 1, 1),\n",
    "'tv_weight' : 200, \n",
    "'content_layer' : 'block4_conv2', \n",
    "'style_layers' : ['block1_conv2',\n",
    "                  'block2_conv2',\n",
    "                  'block3_conv4', \n",
    "                  'block4_conv4', \n",
    "                  'block5_conv4'], \n",
    "'iterations' : 50\n",
    "}\n",
    "\n",
    "style_transfer(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pavlos_grandma](images/results/pavlos_grandma_umbrella_50.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgments\n",
    "- The implementation uses code from Francois Chollet's neural style transfer.\n",
    "- The implementation uses code from Kevin Zakka's neural style transfer, under MIT license.\n",
    "- The hierarchy borrows from Giuseppe Bonaccorso's gist, under MIT license.\n",
    "- Some of the documentation guidelines and function documentation have been borrowed from Stanford's cs231n course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

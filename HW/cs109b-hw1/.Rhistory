bathrooms = j_fold_data$bathrooms,
bedrooms = j_fold_data$bedrooms,
beds = j_fold_data$beds,
security_deposit = j_fold_data$security_deposit,
cleaning_fee = j_fold_data$cleaning_fee,
availability_365 = j_fold_data$availability_365,
number_of_reviews = j_fold_data$number_of_reviews,
room_type = j_fold_data$room_type))
# Compute R^2 for predicted values
cv_rsq[i] = cv_rsq[i] + rsq(train$price[folds == j], pred)
}
# Average R^2 across k folds
cv_rsq[i] = cv_rsq[i] / k
}
# Return cross-validated R^2 values
return(cv_rsq)
}
spars <- seq(0.1, 1, by = 0.1)
cv_gam_scores <- cv_gam(df_train2, spars, 5)
opt_spar <- spars[which.max(cv_gam_scores)]
cv_gam_scores
opt_spar
# Refit on best spar value
model.formula.gam_opt <- as.formula(paste("price ~ ",
"s(host_total_listings_count, spar = ", opt_spar,") +",
"s(latitude, spar = ", opt_spar,") + ",
"s(longitude, spar = ", opt_spar,") + ",
"s(bathrooms, spar = ", opt_spar,") + ",
"s(bedrooms, spar = ", opt_spar,") + ",
"s(beds, spar = ", opt_spar,") + ",
"s(security_deposit, spar = ", opt_spar,") + ",
"s(cleaning_fee, spar = ", opt_spar,") + ",
"s(availability_365, spar = ", opt_spar,") + ",
"s(number_of_reviews, spar = ", opt_spar,") + room_type"))
model.gam_opt <- gam(formula = model.formula.gam_opt, data = df_train2)
pred.gam_opt.train <- predict(model.gam_opt)
pred.gam_opt.test <- predict(model.gam_opt, df_test2)
train.r2.gam_opt <- rsq(df_train2$price, pred.gam_opt.train)
test.r2.gam_opt <- rsq(df_test2$price, pred.gam_opt.test)
train.r2.gam_opt
test.r2.gam_opt
# Problem 2b part 2
plot(model.gam_opt, se = TRUE)
summary(model.gam_opt)
# Problem 2b part 3
# Compare GAM and linear regression model
anova(model.linear, model.gam_opt)
cv_gam2 = function(train, param_val, k) {
# Input:
#   Training data frame: 'train',
#   Vector of spar parameter values: 'param_val',
#   Number of CV folds: 'k'
# Output:
#   Vector of R^2 values for the provided parameters: 'cv_rsq'
num_param = length(param_val) # Number of parameters
set.seed(109) # Set seed for random number generator
# Divide training set into k folds by sampling uniformly at random
# folds[s] has the fold index for train instance 's'
folds = sample(1:k, nrow(train), replace = TRUE)
cv_rsq = rep(0., num_param) # Store cross-validated R^2 for different parameter values
# Iterate over parameter values
for(i in 1:num_param){
# Iterate over folds to compute R^2 for parameter
for(j in 1:k){
# Fit model on all folds other than 'j' with parameter value param_val[i]
model.formula = as.formula(paste("price ~ ",
"s(host_total_listings_count, spar = ", param_val[i],") +",
"s(latitude, spar = ", param_val[i],") + ",
"s(longitude, spar = ", param_val[i],") + ",
"s(bathrooms, spar = ", param_val[i],") + ",
"s(bedrooms, spar = ", param_val[i],") + ",
"s(beds, spar = ", param_val[i],") + ",
"s(security_deposit, spar = ", param_val[i],") + ",
"s(cleaning_fee, spar = ", param_val[i],") + room_type"))
model.gam = gam(formula = model.formula, data = train[folds!=j, ])
# Make prediction on fold 'j'
j_fold_data = train[folds == j,]
pred = predict(model.gam,
data.frame(
host_total_listings_count = j_fold_data$host_total_listings_count,
latitude = j_fold_data$latitude,
longitude = j_fold_data$longitude,
bathrooms = j_fold_data$bathrooms,
bedrooms = j_fold_data$bedrooms,
beds = j_fold_data$beds,
security_deposit = j_fold_data$security_deposit,
cleaning_fee = j_fold_data$cleaning_fee,
room_type = j_fold_data$room_type))
# Compute R^2 for predicted values
cv_rsq[i] = cv_rsq[i] + rsq(train$price[folds == j], pred)
}
# Average R^2 across k folds
cv_rsq[i] = cv_rsq[i] / k
}
# Return cross-validated R^2 values
return(cv_rsq)
}
cv_gam_scores2 <- cv_gam(df_train2, spars, 5)
opt_spar2 <- spars[which.max(cv_gam_scores2)]
cv_gam_scores2
opt_spar2
# Refit on best spar value
model.formula.gam_opt2 <- as.formula(paste("price ~ ",
"s(host_total_listings_count, spar = ", opt_spar2,") +",
"s(latitude, spar = ", opt_spar2,") + ",
"s(longitude, spar = ", opt_spar2,") + ",
"s(bathrooms, spar = ", opt_spar2,") + ",
"s(bedrooms, spar = ", opt_spar2,") + ",
"s(beds, spar = ", opt_spar2,") + ",
"s(security_deposit, spar = ", opt_spar2,") + ",
"s(cleaning_fee, spar = ", opt_spar2,") + room_type"))
model.gam_opt2 <- gam(formula = model.formula.gam_opt2, data = df_train2)
pred.gam_opt.train2 <- predict(model.gam_opt2)
pred.gam_opt.test2 <- predict(model.gam_opt2, df_test2)
train.r2.gam_opt2 <- rsq(df_train2$price, pred.gam_opt.train2)
test.r2.gam_opt2 <- rsq(df_test2$price, pred.gam_opt.test2)
train.r2.gam_opt2
test.r2.gam_opt2
anova(model.gam_opt2, model.gam_opt)
df_train2_cols <- names(df_train2)
features <- c('host_total_listings_count', 'latitude', 'longitude', 'bathrooms', 'bedrooms', 'beds', 'security_deposit', 'cleaning_fee', 'availability_365', 'number_of_reviews', 'room_type')
# features <- df_train2_cols[df_train2_cols != 'price']
n_features <- ncol(df_train2) - 1
f_all <- list()
beta <- mean(df_train2$price) # initialize beta to be the mean of y's
f_matrix <- matrix(rep(0, n_features*nrow(df_train2)), ncol = n_features)
for (i in 1:8) {
for (j in 1:n_features) {
other_pred <- rowSums(f_matrix) - f_matrix[,j]
if (j == 11) {
f_all[[j]] <- lm(response ~ predictor,
data = data.frame(predictor = df_train2$room_type, response = as.matrix(df_train2$price - beta - other_pred)))
pred_j <- as.matrix(predict(f_all[[j]]))
}
else {
f_all[[j]] = smooth.spline(x = as.matrix(df_train2[features[j]]), y = as.matrix(df_train2$price - beta - other_pred),
tol=1e-10, spar = opt_spar)
pred_j <- as.matrix(predict(f_all[[j]], df_train2[features[j]])$y)
}
f_matrix[,j] <- pred_j - mean(pred_j)
}
}
# evalurate R^2
pred.backfit.ss.train <- rep(beta, nrow(df_train2))
pred.backfit.ss.test <- rep(beta, nrow(df_test2))
for (j in 1:n_features) {
if (j == 11) {
pred.backfit.ss.train <- pred.backfit.ss.train + as.matrix(predict(f_all[[j]], data.frame(predictor = df_train2$room_type)))
pred.backfit.ss.test <- pred.backfit.ss.test + as.matrix(predict(f_all[[j]], data.frame(predictor = df_test2$room_type)))
}
else {
pred.backfit.ss.train <- pred.backfit.ss.train + as.matrix(predict(f_all[[j]], df_train2[features[j]])$y)
pred.backfit.ss.test <- pred.backfit.ss.test + as.matrix(predict(f_all[[j]], df_test2[features[j]])$y)
}
}
pred.backfit.ss.train.r2 <- rsq(df_train2$price, pred.backfit.ss.train)
pred.backfit.ss.test.r2 <- rsq(df_test2$price, pred.backfit.ss.test)
print(paste0("Backfit train R2 = ", pred.backfit.ss.train.r2))
print(paste0("Backfit test R2 = ", pred.backfit.ss.test.r2))
print(paste0("GAM train R2 = ", train.r2.gam_opt))
print(paste0("GAM test R2 = ", test.r2.gam_opt))
summary(model.gam_opt)
df_train2_cols <- names(df_train2)
features <- c('host_total_listings_count', 'latitude', 'longitude', 'bathrooms', 'bedrooms', 'beds', 'security_deposit', 'cleaning_fee', 'availability_365', 'number_of_reviews', 'room_type')
# features <- df_train2_cols[df_train2_cols != 'price']
n_features <- ncol(df_train2) - 1
f_all <- list() # the list of basis spline functions for each predictor
beta <- mean(df_train2$price) # initialize beta to be the mean of y's
# the matrix that records the predictions of each basis function
f_matrix <- matrix(rep(0, n_features*nrow(df_train2)), ncol = n_features)
for (i in 1:8) {
for (j in 1:n_features) {
other_pred <- rowSums(f_matrix) - f_matrix[,j]
if (j == 11) {
f_all[[j]] <- lm(response ~ predictor,
data = data.frame(predictor = df_train2$room_type,
response = as.matrix(df_train2$price - beta - other_pred)))
pred_j <- as.matrix(predict(f_all[[j]]))
}
else {
f_all[[j]] = smooth.spline(x = as.matrix(df_train2[features[j]]),
y = as.matrix(df_train2$price - beta - other_pred),
tol=1e-10, spar = opt_spar)
pred_j <- as.matrix(predict(f_all[[j]], df_train2[features[j]])$y)
}
f_matrix[,j] <- pred_j - mean(pred_j)
}
}
# evalurate R^2 and compare with GAM
pred.backfit.ss.train <- rep(beta, nrow(df_train2))
pred.backfit.ss.test <- rep(beta, nrow(df_test2))
for (j in 1:n_features) {
if (j == 11) {
pred.backfit.ss.train <- pred.backfit.ss.train + as.matrix(predict(f_all[[j]],
data.frame(predictor = df_train2$room_type)))
pred.backfit.ss.test <- pred.backfit.ss.test + as.matrix(predict(f_all[[j]],
data.frame(predictor = df_test2$room_type)))
}
else {
pred.backfit.ss.train <- pred.backfit.ss.train + as.matrix(predict(f_all[[j]],
df_train2[features[j]])$y)
pred.backfit.ss.test <- pred.backfit.ss.test + as.matrix(predict(f_all[[j]],
df_test2[features[j]])$y)
}
}
# Function to compute the sum of squared different between y1 and y2
sum_sqr_diff = function(y1, y2) {
return (sum((y1 - y2)^2))
}
pred.backfit.ss.train.r2 <- rsq(df_train2$price, pred.backfit.ss.train)
pred.backfit.ss.test.r2 <- rsq(df_test2$price, pred.backfit.ss.test)
print(paste0("Backfit train R2 = ", pred.backfit.ss.train.r2))
print(paste0("Backfit test R2 = ", pred.backfit.ss.test.r2))
print(paste0("GAM train R2 = ", train.r2.gam_opt))
print(paste0("GAM test R2 = ", test.r2.gam_opt))
print(paste0("Sum of Squared Different between the predictions of backfitting and GAM - train",
sum_sqr_diff(pred.backfit.ss.train, pred.gam_opt.train)))
print(paste0("Sum of Squared Different between the predictions of backfitting and GAM - test",
sum_sqr_diff(pred.backfit.ss.test, pred.gam_opt.test)))
df_train2_cols <- names(df_train2)
features <- c('host_total_listings_count', 'latitude', 'longitude', 'bathrooms', 'bedrooms', 'beds', 'security_deposit', 'cleaning_fee', 'availability_365', 'number_of_reviews', 'room_type')
# features <- df_train2_cols[df_train2_cols != 'price']
n_features <- ncol(df_train2) - 1
f_all <- list() # the list of basis spline functions for each predictor
beta <- mean(df_train2$price) # initialize beta to be the mean of y's
# the matrix that records the predictions of each basis function
f_matrix <- matrix(rep(0, n_features*nrow(df_train2)), ncol = n_features)
for (i in 1:8) {
for (j in 1:n_features) {
other_pred <- rowSums(f_matrix) - f_matrix[,j]
if (j == 11) {
f_all[[j]] <- lm(response ~ predictor,
data = data.frame(predictor = df_train2$room_type,
response = as.matrix(df_train2$price - beta - other_pred)))
pred_j <- as.matrix(predict(f_all[[j]]))
}
else {
f_all[[j]] = smooth.spline(x = as.matrix(df_train2[features[j]]),
y = as.matrix(df_train2$price - beta - other_pred),
tol=1e-10, spar = opt_spar)
pred_j <- as.matrix(predict(f_all[[j]], df_train2[features[j]])$y)
}
f_matrix[,j] <- pred_j - mean(pred_j)
}
}
# evalurate R^2 and compare with GAM
pred.backfit.ss.train <- rep(beta, nrow(df_train2))
pred.backfit.ss.test <- rep(beta, nrow(df_test2))
for (j in 1:n_features) {
if (j == 11) {
pred.backfit.ss.train <- pred.backfit.ss.train + as.matrix(predict(f_all[[j]],
data.frame(predictor = df_train2$room_type)))
pred.backfit.ss.test <- pred.backfit.ss.test + as.matrix(predict(f_all[[j]],
data.frame(predictor = df_test2$room_type)))
}
else {
pred.backfit.ss.train <- pred.backfit.ss.train + as.matrix(predict(f_all[[j]],
df_train2[features[j]])$y)
pred.backfit.ss.test <- pred.backfit.ss.test + as.matrix(predict(f_all[[j]],
df_test2[features[j]])$y)
}
}
# Function to compute the sum of squared different between y1 and y2
sum_sqr_diff = function(y1, y2) {
return (sum((y1 - y2)^2))
}
pred.backfit.ss.train.r2 <- rsq(df_train2$price, pred.backfit.ss.train)
pred.backfit.ss.test.r2 <- rsq(df_test2$price, pred.backfit.ss.test)
print(paste0("Backfit train R2 = ", pred.backfit.ss.train.r2))
print(paste0("Backfit test R2 = ", pred.backfit.ss.test.r2))
print(paste0("GAM train R2 = ", train.r2.gam_opt))
print(paste0("GAM test R2 = ", test.r2.gam_opt))
print(paste0("Sum of Squared Different between the predictions of backfitting and GAM - train = ",
sum_sqr_diff(pred.backfit.ss.train, pred.gam_opt.train)))
print(paste0("Sum of Squared Different between the predictions of backfitting and GAM - test = ",
sum_sqr_diff(pred.backfit.ss.test, pred.gam_opt.test)))
df_train2_cols <- names(df_train2)
features <- c('host_total_listings_count', 'latitude', 'longitude', 'bathrooms', 'bedrooms', 'beds', 'security_deposit', 'cleaning_fee', 'availability_365', 'number_of_reviews', 'room_type')
# features <- df_train2_cols[df_train2_cols != 'price']
n_features <- ncol(df_train2) - 1
f_all <- list() # the list of basis spline functions for each predictor
beta <- mean(df_train2$price) # initialize beta to be the mean of y's
# the matrix that records the predictions of each basis function
f_matrix <- matrix(rep(0, n_features*nrow(df_train2)), ncol = n_features)
for (i in 1:8) {
for (j in 1:n_features) {
other_pred <- rowSums(f_matrix) - f_matrix[,j]
if (j == 11) {
f_all[[j]] <- lm(response ~ predictor,
data = data.frame(predictor = df_train2$room_type,
response = as.matrix(df_train2$price - beta - other_pred)))
pred_j <- as.matrix(predict(f_all[[j]]))
}
else {
f_all[[j]] = smooth.spline(x = as.matrix(df_train2[features[j]]),
y = as.matrix(df_train2$price - beta - other_pred),
tol=1e-10, spar = opt_spar)
pred_j <- as.matrix(predict(f_all[[j]], df_train2[features[j]])$y)
}
f_matrix[,j] <- pred_j - mean(pred_j)
}
}
# evalurate R^2 and compare with GAM
pred.backfit.ss.train <- rep(beta, nrow(df_train2))
pred.backfit.ss.test <- rep(beta, nrow(df_test2))
for (j in 1:n_features) {
if (j == 11) {
pred.backfit.ss.train <- pred.backfit.ss.train + as.matrix(predict(f_all[[j]],
data.frame(predictor = df_train2$room_type)))
pred.backfit.ss.test <- pred.backfit.ss.test + as.matrix(predict(f_all[[j]],
data.frame(predictor = df_test2$room_type)))
}
else {
pred.backfit.ss.train <- pred.backfit.ss.train + as.matrix(predict(f_all[[j]],
df_train2[features[j]])$y)
pred.backfit.ss.test <- pred.backfit.ss.test + as.matrix(predict(f_all[[j]],
df_test2[features[j]])$y)
}
}
# Function to compute the sum of squared different between y1 and y2
sum_sqr_diff = function(y1, y2) {
return (sum((y1 - y2)^2))
}
pred.backfit.ss.train.r2 <- rsq(df_train2$price, pred.backfit.ss.train)
pred.backfit.ss.test.r2 <- rsq(df_test2$price, pred.backfit.ss.test)
print(paste0("Backfit train R2 = ", pred.backfit.ss.train.r2))
print(paste0("Backfit test R2 = ", pred.backfit.ss.test.r2))
print(paste0("GAM train R2 = ", train.r2.gam_opt))
print(paste0("GAM test R2 = ", test.r2.gam_opt))
print(rsq(pred.backfit.ss.train, pred.gam_opt.train))
print(rsq(pred.backfit.ss.test, pred.gam_opt.test))
# print(paste0("Sum of Squared Different between the predictions of backfitting and GAM - train = ",
#              sum_sqr_diff(pred.backfit.ss.train, pred.gam_opt.train)))
# print(paste0("Sum of Squared Different between the predictions of backfitting and GAM - test = ",
#              sum_sqr_diff(pred.backfit.ss.test, pred.gam_opt.test)))
# import libraries
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(splines)
# read in data
df_train <- read.csv("data/calendar_train.csv")
df_test <- read.csv("data/calendar_test.csv")
# convert dates into date object
df_train$date <- as.Date(df_train$date,"%m/%d/%y")
df_test$date <- as.Date(df_test$date, "%m/%d/%y")
# exclude observations that are missing price
df_train <- df_train[!is.na(df_train$price),]
df_test <- df_test[!is.na(df_test$price), ]
# group df_train by month or by day of week and calculate the mean price
mean_price_by_month <- df_train %>% group_by(month = factor(format(df_train$date, '%m'))) %>%
summarise(mean_price = mean(price))
mean_price_by_day_of_week <- df_train %>% group_by(day_of_week = factor(weekdays(date))) %>%
summarise(mean_price = mean(price))
# relevel day of week from Monday to Sunday
mean_price_by_day_of_week$day_of_week <- factor(mean_price_by_day_of_week$day_of_week,
levels = c("Monday", "Tuesday", "Wednesday", "Thursday",
"Friday", "Saturday", "Sunday"))
# plot
price_by_month.plot <- ggplot(data=mean_price_by_month, aes(x=month, y=mean_price)) +
geom_bar(stat="identity") + ggtitle("Average price by month") + coord_flip()
price_by_day_of_week.plot <- ggplot(data=mean_price_by_day_of_week, aes(x=day_of_week, y=mean_price)) +
geom_bar(stat="identity") + ggtitle("Average price by day of week") + coord_flip()
grid.arrange(price_by_month.plot, price_by_day_of_week.plot, ncol=2)
knitr::opts_chunk$set(echo = TRUE)
# read in data
trump_tibble <- read.csv("trump_tibble.csv")
# cast factors to characters
trump_tibble$document <- as.character(trump_tibble$document)
trump_tibble$text <- as.character(trump_tibble$text)
# load libraries
library(dplyr)
library(topicmodels) #topic modeling functions
library(stringr) #common string functions
library(tidytext) #tidy text analysis
suppressMessages(library(tidyverse)) #data manipulation and visualization
#messages give R markdown compile error so we need to suppress it
## Source topicmodels2LDAvis & optimal_k functions
invisible(lapply(file.path("https://raw.githubusercontent.com/trinker/topicmodels_learning/master/functions",
c("topicmodels2LDAvis.R", "optimal_k.R")),
devtools::source_url))
df_trump_words <- trump_tibble %>% unnest_tokens(word, text)
head(df_trump_words)
?summarise
df_trump_word_count <- df_trump_words %>% group_by(factor(word)) %>% summarise(word_count = n_distinct(document))
df_trump_word_count
df_trump_word_count <- df_trump_words %>% group_by(word = word) %>% summarise(word_count = n_distinct(document))
head(df_trump_word_count)
?cast-dtm
?cast_dtm
tweet_dtm <- cast-dtm(df_trump_word_count)
tweet_dtm <- cast_dtm(df_trump_word_count)
df_trump_word_count <- df_trump_words %>% group_by(word = word) %>% summarise(word_count = n_distinct(document))
head(df_trump_word_count)
df_trump_term_count <- df_trump_words %>% group_by(word) %>% summarise(count = n_distinct(document)) %>% left_join(df_trump_words, ., by = word) %>%
select(document, word, count) %>%
head()
df_trump_term_count <- df_trump_words %>% group_by(word) %>% summarise(count = n_distinct(document)) %>% left_join(df_trump_words, ., by = 'word') %>%
select(document, word, count) %>%
head()
df_trump_word_count <- df_trump_words %>% group_by(word) %>% summarise(count = n_distinct(document)) %>% left_join(df_trump_words, ., by = 'word') %>%
select(document, word, count)
head(df_trump_word_count)
tweet_dtm <- cast_dtm(df_trump_word_count, document, word, count)
head(tweet_dtm)
tweet_dtm <- df_trump_word_count %>% cast_dtm(document, word, count)
head(tweet_dtm)
tweet_dtm <- cast_dtm(df_trump_word_count, df_trump_word_count$document, df_trump_word_count$word,
df_trump_word_count$count)
tweet_dtm <- cast_dtm(df_trump_words, df_trump_word_count$document, df_trump_word_count$word,
df_trump_word_count$count)
?tidy
library(tidytext)
?tidy
tidy(df_trump_word_count)
tweet_dtm <- cast_dtm(tidy(df_trump_word_count), df_trump_word_count$document, df_trump_word_count$word,
df_trump_word_count$count)
tweet_dtm <- cast_dtm(df_trump_word_count, df_trump_word_count$document, df_trump_word_count$word,
df_trump_word_count$count)
rowSums(is.na(df_trump_word_count))
tweet_dtm <- df_trump_word_count %>% cast_dtm(document, word, count)
head(tweet_dtm)
factor(df_trump_word_count$document)
levels(df_trump_word_count$document)
levels(as.factor(df_trump_word_count$document))
tweet_dtm <- df_trump_word_count %>% cast_dtm(document, word, count)
head(tweet_dtm)
df_by_word <- trump_tibble %>% unnest_tokens(word, text)
head(df_by_word)
# split into words
tweet_by_word <- trump_tibble %>% unnest_tokens(word, text)
head(tweet_by_word)
# find document-word counts, excluding stop words ("the"", "as", "and", "of")
word_counts <- tweet_by_word %>%
anti_join(stop_words) %>%
count(document, word, sort = TRUE) %>%
ungroup()
top_n(word_counts, 10)
#
# df_trump_word_count <- df_trump_words %>% group_by(word) %>% summarise(count = n_distinct(document)) %>% left_join(df_trump_words, ., by = 'word') %>%
# select(document, word, count)
# head(df_trump_word_count)
tweet_dtm <- word_counts %>% cast_dtm(document, word, n)
tweet_dtm
?LDA
?optimal_k
??optimal_k
?LDA
?optimal-k
control <- list(burnin = 500, iter = 1000, keep = 100, seed = 46)
opt.k = optimal_k(tweet_dtm, max.k=30, control=control)
opt.k
?optimal_k
opt.k
?tidy
tweet_lda <- LDA(tweet_dtm, k = as.numeric(opt.k), method="Gibbs", control = control)
top_n(tweet_topics, 10)
tweet_topics <- tidy(chapters_lda, matrix = "beta")
tweet_topics <- tidy(tweet_lda, matrix = "beta")
top_n(tweet_topics, 10)
lda_inf = posterior(tweet_lda)
lda_inf
topics.hp = topics(tweet_lda, 1)
topics.hp
terms.hp = terms(tweet_lda, 10)
print(terms.hp[,1:10])
top_n(tweet_topics, 10)
tweet_topics
top_terms <- tweet_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
tweet_topics %>% group_by(topic) %>% top_n(10, beta) %>%
ungroup() %>%  arrange(topic, -beta)
tweet_lda <- LDA(tweet_dtm, k = as.numeric(opt.k), method="Gibbs", control = control)
tweet_topics <- tidy(tweet_lda, matrix = "beta")
# lda_inf = posterior(tweet_lda)
# topics.hp = topics(tweet_lda, 1)
# terms.hp = terms(tweet_lda, 10)
# print(terms.hp[,1:10])
# print out the top 10 words for each of the k topics
top_terms <- tweet_topics %>% group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
tweet_lda <- LDA(tweet_dtm, k = as.numeric(opt.k), method="Gibbs", control = control)
tweet_topics <- tidy(tweet_lda, matrix = "beta")
# lda_inf = posterior(tweet_lda)
# topics.hp = topics(tweet_lda, 1)
# terms.hp = terms(tweet_lda, 10)
# print(terms.hp[,1:10])
# print out the top 10 words for each of the k topics
top_terms <- tweet_topics %>% group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
print(top_terms)
